{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T21:13:58.182292Z",
     "start_time": "2025-05-16T21:13:52.440539Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from common.helpers import logger, has_nvidia_gpu, display_feature_importances\n",
    "from common.feature_engineering import feature_engineering\n",
    "from common.imputation import get_imputation_values, apply_imputation"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:31:49.106550Z",
     "start_time": "2025-05-16T21:31:49.079816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration ---\n",
    "HAS_GPU = False # has_nvidia_gpu() # False #HAS_GPU = cp.cuda.runtime.getDeviceCount() > 0\n",
    "FOLD_AMOUNT = 3\n",
    "TESTSPLIT_RATIO = 10 # Percentage of data to be used for testing\n",
    "OPTUNA_TRIALS = 2 #20 # Number of trials for hyperparameter optimization\n",
    "ENSEMBLE_N_ESTIMATORS = 2 #50 # Number of estimators for the final stacking model\n",
    "TRAIN_WITHOUT_EVALUATION = False # If we should train without evaluation, gives more training data but can't output evaluation metrics\n",
    "TRAIN_DATA_PERCENTAGE = 1 # Percentage of train data to use for the training, 1 for everything (100%).\n",
    "TEST_DATA_PERCENTAGE = 1 # Percentage of test data to use for the training, 1 for everything (100%).\n",
    "categorical_feature = ['site_id', 'visitor_location_country_id', 'prop_country_id']"
   ],
   "id": "f4d882c195c3b847",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:34:28.873685Z",
     "start_time": "2025-05-16T21:32:12.378267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load the data ---\n",
    "training_data_path = r\"../input/training_set_VU_DM.csv\"\n",
    "test_data_path = r\"../input/test_set_VU_DM.csv\"\n",
    "# df_ori = pd.read_csv(training_data_path)\n",
    "# df_test_ori = pd.read_csv(test_data_path)\n",
    "df = pd.read_csv(training_data_path).sample(frac=TRAIN_DATA_PERCENTAGE, random_state=42)\n",
    "df_test = pd.read_csv(test_data_path).sample(frac=TEST_DATA_PERCENTAGE, random_state=42)\n",
    "# df = df_ori.sample(frac=TRAIN_DATA_PERCENTAGE, random_state=42)\n",
    "# df_test = df_test_ori.sample(frac=TEST_DATA_PERCENTAGE, random_state=42)"
   ],
   "id": "6fa538012884ad37",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:35:04.295093Z",
     "start_time": "2025-05-16T21:35:04.278332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask = (data[column].between(lower_bound, upper_bound)) | (data[column].isna())\n",
    "    df_cleaned = df[mask].copy()\n",
    "    return df_cleaned\n",
    "\n",
    "def remove_outliers_percentile(data, column, lower_pct=0.01, upper_pct=0.99):\n",
    "    lower = data[column].quantile(lower_pct)\n",
    "    upper = data[column].quantile(upper_pct)\n",
    "    df_cleaned = data[(data[column] >= lower) & (data[column] <= upper)].copy()\n",
    "    return df_cleaned"
   ],
   "id": "b8dc23a3a14d76ea",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:35:06.986035Z",
     "start_time": "2025-05-16T21:35:06.977789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_diff_from_group_mean(data, group_col='srch_id',value_col='prop_starrating'):\n",
    "    data_tmp = data.copy()\n",
    "    mean_col_name = f'{value_col}_mean_by_{group_col}'\n",
    "    data_tmp[mean_col_name] = data_tmp.groupby(group_col)[value_col].transform('mean')\n",
    "\n",
    "    diff_col_name = f'{value_col}_diff_from_mean'\n",
    "    data_tmp[diff_col_name] = data_tmp[value_col] - data_tmp[mean_col_name]\n",
    "    return data_tmp"
   ],
   "id": "1702f02e65ec6c18",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:49:45.722423Z",
     "start_time": "2025-05-16T11:49:45.701033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_imputation_values(train_data):\n",
    "\n",
    "    # Impute missing values with median or specific values\n",
    "    impute_values = {\n",
    "        \"visitor_hist_starrating\": train_data[\"visitor_hist_starrating\"].median(),\n",
    "        \"visitor_hist_adr_usd\": train_data[\"visitor_hist_adr_usd\"].median(),\n",
    "        \"prop_review_score\": 0,\n",
    "        \"prop_location_score2\": 0,\n",
    "        \"srch_query_affinity_score\": train_data[\"srch_query_affinity_score\"].min(),\n",
    "        \"orig_destination_distance\": train_data[\"orig_destination_distance\"].median()\n",
    "    }\n",
    "\n",
    "    return impute_values\n",
    "\n",
    "def apply_imputation(data, impute_values):\n",
    "    data_impute = data.copy()\n",
    "\n",
    "    data_impute[\"visitor_hist_starrating\"] = data_impute[\"visitor_hist_starrating\"].fillna(impute_values[\"visitor_hist_starrating\"])\n",
    "    data_impute[\"visitor_hist_adr_usd\"] = data_impute[\"visitor_hist_adr_usd\"].fillna(impute_values[\"visitor_hist_adr_usd\"])\n",
    "    data_impute[\"prop_review_score\"] = data_impute[\"prop_review_score\"].fillna(impute_values[\"prop_review_score\"])\n",
    "    data_impute[\"prop_location_score2\"] = data_impute[\"prop_location_score2\"].fillna(impute_values[\"prop_location_score2\"])\n",
    "    data_impute[\"srch_query_affinity_score\"] = data_impute[\"srch_query_affinity_score\"].fillna(impute_values[\"srch_query_affinity_score\"])\n",
    "    data_impute[\"orig_destination_distance\"] = data_impute[\"orig_destination_distance\"].fillna(impute_values[\"orig_destination_distance\"])\n",
    "\n",
    "    logger.debug(\"Imputation finished.\")\n",
    "    return data_impute"
   ],
   "id": "f113b9cc7012b7d3",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:35:11.407380Z",
     "start_time": "2025-05-16T21:35:11.357946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_PRICE_NIGHT = 150000\n",
    "\n",
    "# Generate additional features that may be useful for the model\n",
    "def feature_engineering(data, type='train'):\n",
    "    logger.debug(\"Running feature engineering\")\n",
    "\n",
    "    # Delete rows gross_bookings_usd is significantly different from price_usd\n",
    "    if type=='train':\n",
    "        data['booking_price_diff'] = data['gross_bookings_usd'] - data['price_usd']\n",
    "        data = remove_outliers_iqr(data=data, column='booking_price_diff')\n",
    "        data.drop(columns=[\"booking_price_diff\"], inplace=True)\n",
    "\n",
    "    # transfer date to month and day of week\n",
    "    data['date_time'] = pd.to_datetime(data['date_time'], errors='coerce')\n",
    "    data['month'] = data['date_time'].dt.month\n",
    "    data['dayofweek'] = data['date_time'].dt.dayofweek\n",
    "    data.drop(columns=[\"date_time\"], inplace=True)\n",
    "\n",
    "    # Feature for total number of adults and children\n",
    "    data[\"total_people\"] = data[\"srch_adults_count\"] + data[\"srch_children_count\"]\n",
    "\n",
    "    # History differences\n",
    "    data[\"history_starrating_diff\"] = data[\"visitor_hist_starrating\"] - data[\"prop_starrating\"]\n",
    "\n",
    "    # Total price per night per room\n",
    "    data['price_1room_1night'] = (data['price_usd'] / data['srch_room_count']) / data['srch_length_of_stay']\n",
    "    data[\"history_adr_diff\"] = data[\"visitor_hist_adr_usd\"] - data[\"price_1room_1night\"]\n",
    "\n",
    "    # data.drop(columns=['price_usd'], inplace=True)\n",
    "\n",
    "    # Filter out high prices only for train data\n",
    "    if type=='train':\n",
    "        data = data[data['price_1room_1night'] < MAX_PRICE_NIGHT].copy()\n",
    "\n",
    "    # log transform\n",
    "    data['price_1room_1night_log'] = np.log1p(data['price_1room_1night'])\n",
    "    data[\"price_history_difference\"] = data[\"prop_log_historical_price\"] - data[\"price_1room_1night_log\"]\n",
    "    data['price_1person_1night'] = (data['price_usd'] / data['total_people']) / data['srch_length_of_stay']\n",
    "    data.drop(columns=['price_1room_1night'], inplace=True)\n",
    "\n",
    "    # log transform price, min price is 0, so we use log(x+1)\n",
    "    data['visitor_hist_adr_usd_log'] = np.log1p(data['visitor_hist_adr_usd'])\n",
    "    data.drop(columns=['visitor_hist_adr_usd'], inplace=True)\n",
    "\n",
    "    # Transformations of competitor rates\n",
    "    data[\"avg_comp_rate\"] = data[[\"comp1_rate\", \"comp2_rate\", \"comp3_rate\", \"comp4_rate\", \"comp5_rate\", \"comp6_rate\", \"comp7_rate\", \"comp8_rate\"]].sum(axis=1)\n",
    "    data[\"avg_comp_inv\"] = data[[\"comp1_inv\", \"comp2_inv\", \"comp3_inv\", \"comp4_inv\", \"comp5_inv\", \"comp6_inv\", \"comp7_inv\", \"comp8_inv\"]].sum(axis=1)\n",
    "    data[\"avg_comp_rate_percent_diff\"] = data[[\"comp1_rate_percent_diff\", \"comp2_rate_percent_diff\", \"comp3_rate_percent_diff\", \"comp4_rate_percent_diff\", \"comp5_rate_percent_diff\", \"comp6_rate_percent_diff\", \"comp7_rate_percent_diff\", \"comp8_rate_percent_diff\"]].dropna().median(axis=1)\n",
    "\n",
    "    # Locational features\n",
    "    data[\"domestic_travel_bool\"] = data[\"prop_country_id\"] == data[\"visitor_location_country_id\"]\n",
    "\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_starrating')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='prop_id',value_col='price_1room_1night_log')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_location_score1')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_location_score2')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_destination_id',value_col='price_usd')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_review_score')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='promotion_flag')\n",
    "\n",
    "    # # drop original competitor columns\n",
    "    # for x in range(1, 9):\n",
    "    #     data.drop(columns=[f\"comp{x}_rate\", f\"comp{x}_inv\", f\"comp{x}_rate_percent_diff\"], inplace=True, errors='ignore')\n",
    "\n",
    "    data.drop(columns=['srch_destination_id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for c in categorical_feature:\n",
    "        data[c] = data[c].astype('category')\n",
    "\n",
    "    logger.debug(\"Feature engineering completed\")\n",
    "    return data"
   ],
   "id": "c6e833395965310d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:42:46.518937Z",
     "start_time": "2025-05-16T21:35:17.594598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine imputation values on training set\n",
    "# impute_values = get_imputation_values(df)\n",
    "\n",
    "# Apply the same imputation on training and test\n",
    "# df = apply_imputation(df, impute_values)\n",
    "df = feature_engineering(data=df,type='train')\n",
    "\n",
    "# NOTE ASSIGNMENT PROVIDED TEST SET CONTAINS NO CLICK_BOOL THUS USELESS FOR TESTING\n",
    "# df_test = apply_imputation(df_test, impute_values)\n",
    "df_test = feature_engineering(data=df_test,type='test')\n"
   ],
   "id": "a3d834ec693f14e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 23:35:17,597 - DEBUG - Running feature engineering\n",
      "2025-05-16 23:39:25,503 - DEBUG - Feature engineering completed\n",
      "2025-05-16 23:39:26,214 - DEBUG - Running feature engineering\n",
      "2025-05-16 23:42:46,354 - DEBUG - Feature engineering completed\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:48:20.915700Z",
     "start_time": "2025-05-16T21:46:46.237456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Split df into training and validation sets based on srch_id ---\n",
    "all_srch_ids = df['srch_id'].unique()\n",
    "train_srch_ids, val_srch_ids = train_test_split(all_srch_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "train_split_df = df[df['srch_id'].isin(train_srch_ids)].copy()\n",
    "val_df = df[df['srch_id'].isin(val_srch_ids)].copy()\n",
    "\n",
    "print(f\"\\nShape of train_split_df: {train_split_df.shape}\")\n",
    "print(f\"Shape of val_df: {val_df.shape}\")\n",
    "print(f\"Number of unique srch_id in train_split_df: {train_split_df['srch_id'].nunique()}\")\n",
    "print(f\"Number of unique srch_id in val_df: {val_df['srch_id'].nunique()}\")\n"
   ],
   "id": "5e07d5b64493293b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of train_split_df: (4449723, 78)\n",
      "Shape of val_df: (495871, 78)\n",
      "Number of unique srch_id in train_split_df: 179808\n",
      "Number of unique srch_id in val_df: 19979\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T21:50:21.716613Z",
     "start_time": "2025-05-16T21:49:27.947697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_cols = [col for col in df.columns if col not in ['srch_id', 'prop_id', 'booking_bool','click_bool', 'position','gross_bookings_usd']]\n",
    "\n",
    "# --- Prepare Training Data ---\n",
    "train_split_df = train_split_df.sort_values('srch_id') # Sort by srch_id\n",
    "X_train = train_split_df[feature_cols]\n",
    "y_train = train_split_df['booking_bool']\n",
    "group_train = train_split_df.groupby('srch_id', sort=False).size().to_list()\n",
    "\n",
    "print(f\"\\nNumber of training groups (searches) for actual training: {len(group_train)}\")\n",
    "print(f\"Total training samples for actual training: {sum(group_train)}\")\n",
    "\n",
    "# --- Prepare Validation Data ---\n",
    "val_df = val_df.sort_values('srch_id') # Sort by srch_id\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['booking_bool']\n",
    "group_val = val_df.groupby('srch_id', sort=False).size().to_list()\n",
    "\n",
    "print(f\"\\nNumber of validation groups (searches): {len(group_val)}\")\n",
    "print(f\"Total validation samples: {sum(group_val)}\")\n",
    "\n",
    "# --- Prepare Test Data  ---\n",
    "X_test = df_test[feature_cols]\n",
    "# We need srch_id and prop_id from test_df for final output generation\n",
    "test_ids = df_test[['srch_id', 'prop_id']].copy()\n",
    "len(X_test)"
   ],
   "id": "785aa8a587dd2daf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training groups (searches) for actual training: 179808\n",
      "Total training samples for actual training: 4449723\n",
      "\n",
      "Number of validation groups (searches): 19979\n",
      "Total validation samples: 495871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4959183"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T22:15:29.553959Z",
     "start_time": "2025-05-16T22:08:19.847902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Train LGBMRanker Model\n",
    "print(\"\\nTraining LGBMRanker model...\")\n",
    "\n",
    "ranker = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\",  # Core objective for learning to rank\n",
    "    metric=\"ndcg\",           # Evaluation metric (Normalized Discounted Cumulative Gain)\n",
    "    n_estimators=1000,        # Number of boosting rounds\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,           # No limit on depth\n",
    "    num_leaves=62,          # Number of leaves in each tree\n",
    "    importance_type='gain',\n",
    "    label_gain=[0, 1],      # The gain for unbooked (0) is 0, and the gain for booked (1) is 1\n",
    "    random_state=42,\n",
    "    n_jobs=-1,        # Use all available cores\n",
    "    categorical_feature=categorical_feature\n",
    "    # Add other parameters as needed, e.g., num_leaves, max_depth, reg_alpha, reg_lambda\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    group=group_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)], # Use validation set here\n",
    "    eval_group=[group_train, group_val],          # Group info for validation set\n",
    "    eval_names=['train', 'valid'],                # Names for the eval sets\n",
    "    eval_at=[5, 10],                              # Evaluate NDCG@k\n",
    "    callbacks=[lgb.early_stopping(200, verbose=True, min_delta=0.0001)] # Adjusted early stopping\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "print(\"\\nFeature Importances:\")\n",
    "feature_importances = pd.Series(ranker.feature_importances_, index=feature_cols)\n",
    "print(feature_importances.sort_values(ascending=False))"
   ],
   "id": "74978c2d5daa40c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LGBMRanker model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiajian/Projects/DataMining/.venv/lib/python3.11/site-packages/lightgbm/basic.py:2137: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(\n",
      "/Users/xiajian/Projects/DataMining/.venv/lib/python3.11/site-packages/lightgbm/basic.py:2159: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] categorical_feature is set=site_id,visitor_location_country_id,prop_country_id, categorical_column=0,1,3 will be ignored. Current value: categorical_feature=site_id,visitor_location_country_id,prop_country_id\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.043026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8635\n",
      "[LightGBM] [Info] Number of data points in the train set: 4449723, number of used features: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiajian/Projects/DataMining/.venv/lib/python3.11/site-packages/lightgbm/basic.py:2137: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(\n",
      "/Users/xiajian/Projects/DataMining/.venv/lib/python3.11/site-packages/lightgbm/basic.py:2159: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Using 0.0001 as min_delta for all metrics.\n",
      "Early stopping, best iteration is:\n",
      "[345]\ttrain's ndcg@5: 0.708683\ttrain's ndcg@10: 0.740337\tvalid's ndcg@5: 0.634967\tvalid's ndcg@10: 0.674243\n",
      "Model training complete.\n",
      "\n",
      "Feature Importances:\n",
      "prop_location_score2                     186086.571016\n",
      "prop_country_id                          109976.917312\n",
      "price_usd                                 96827.574709\n",
      "visitor_location_country_id               84605.188828\n",
      "price_1room_1night_log_diff_from_mean     55515.023365\n",
      "                                             ...      \n",
      "comp7_inv                                   193.197928\n",
      "comp6_rate                                  147.126760\n",
      "random_bool                                 144.237141\n",
      "comp1_inv                                    47.658801\n",
      "avg_comp_rate_percent_diff                    0.000000\n",
      "Length: 72, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T22:18:53.905583Z",
     "start_time": "2025-05-16T22:15:59.063611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Make Predictions on Test Data\n",
    "print(\"\\nPredicting on test data...\")\n",
    "# a NumPy array of scores. Higher scores indicate a higher predicted likelihood of relevance (booking)\n",
    "test_predictions = ranker.predict(X_test)\n",
    "# test_ids = ['srch_id', 'prop_id', 'predicted_score']\n",
    "test_ids['predicted_score'] = test_predictions"
   ],
   "id": "875e8c70208bd0b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on test data...\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T22:18:59.384547Z",
     "start_time": "2025-05-16T22:18:59.336021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(test_predictions)\n",
    "feature_importances"
   ],
   "id": "78393f2e1919b2b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                                5682.025463\n",
       "visitor_location_country_id           84605.188828\n",
       "visitor_hist_starrating                1608.363454\n",
       "prop_country_id                      109976.917312\n",
       "prop_starrating                       21206.869468\n",
       "                                         ...      \n",
       "price_usd_diff_from_mean              48044.876659\n",
       "prop_review_score_mean_by_srch_id      5931.118256\n",
       "prop_review_score_diff_from_mean      18142.572376\n",
       "promotion_flag_mean_by_srch_id         4231.576625\n",
       "promotion_flag_diff_from_mean         28505.691234\n",
       "Length: 72, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T22:20:08.545352Z",
     "start_time": "2025-05-16T22:19:44.158164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Rank Properties and Generate Output File\n",
    "print(\"Ranking properties and generating output file...\")\n",
    "\n",
    "# Sort properties within each search_id group by the predicted score\n",
    "test_ids_sorted = test_ids.sort_values(['srch_id', 'predicted_score'], ascending=[True, False])\n",
    "\n",
    "# Select only srch_id and prop_id for the final output\n",
    "output_df = test_ids_sorted[['srch_id', 'prop_id']]\n",
    "\n",
    "output_filename = \"LGBMRanker_predict.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nOutput file '{output_filename}' generated successfully.\")\n",
    "print(\"Sample of the output file:\")\n",
    "print(output_df.head(10))"
   ],
   "id": "7cb81150acecd010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking properties and generating output file...\n",
      "\n",
      "Output file 'LGBMRanker_predict.csv' generated successfully.\n",
      "Sample of the output file:\n",
      "    srch_id  prop_id\n",
      "23        1    99484\n",
      "9         1    54937\n",
      "12        1    61934\n",
      "5         1    28181\n",
      "4         1    24194\n",
      "6         1    34263\n",
      "22        1    95031\n",
      "20        1    90385\n",
      "8         1    50162\n",
      "18        1    82231\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:53:39.257923Z",
     "start_time": "2025-05-15T15:53:15.538910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load the result ---\n",
    "result_sample = r\"submission_sample.csv\"\n",
    "result = r\"ranked_hotel_bookings.csv\"\n",
    "result_sample = pd.read_csv(result_sample)\n",
    "result = pd.read_csv(result)"
   ],
   "id": "d18f358dae23286e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking properties and generating output file...\n",
      "\n",
      "Output file 'ranked_hotel_bookings.csv' generated successfully.\n",
      "Sample of the output file:\n",
      "    srch_id  prop_id\n",
      "9         1    54937\n",
      "23        1    99484\n",
      "12        1    61934\n",
      "5         1    28181\n",
      "4         1    24194\n",
      "6         1    34263\n",
      "18        1    82231\n",
      "13        1    63894\n",
      "8         1    50162\n",
      "20        1    90385\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:53:46.581529Z",
     "start_time": "2025-05-15T15:53:43.993073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(result))\n",
    "print(len(result_sample))"
   ],
   "id": "2a978ed5f452cddb",
   "outputs": [],
   "execution_count": 91
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
