{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T18:10:00.189664Z",
     "start_time": "2025-05-17T18:09:52.061902Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from common.helpers import logger, has_nvidia_gpu, display_feature_importances\n",
    "from common.feature_engineering import feature_engineering\n",
    "from common.imputation import get_imputation_values, apply_imputation"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:10:07.589391Z",
     "start_time": "2025-05-17T18:10:07.575347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration ---\n",
    "HAS_GPU = False # has_nvidia_gpu() # False #HAS_GPU = cp.cuda.runtime.getDeviceCount() > 0\n",
    "FOLD_AMOUNT = 3\n",
    "TESTSPLIT_RATIO = 10 # Percentage of data to be used for testing\n",
    "OPTUNA_TRIALS = 2 #20 # Number of trials for hyperparameter optimization\n",
    "ENSEMBLE_N_ESTIMATORS = 2 #50 # Number of estimators for the final stacking model\n",
    "TRAIN_WITHOUT_EVALUATION = False # If we should train without evaluation, gives more training data but can't output evaluation metrics\n",
    "TRAIN_DATA_PERCENTAGE = 0.01 # Percentage of train data to use for the training, 1 for everything (100%).\n",
    "TEST_DATA_PERCENTAGE = 0.01 # Percentage of test data to use for the training, 1 for everything (100%).\n",
    "categorical_feature = ['site_id', 'visitor_location_country_id', 'prop_country_id','month','dayofweek']"
   ],
   "id": "f4d882c195c3b847",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:19:43.192685Z",
     "start_time": "2025-05-17T18:17:58.671076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load the data ---\n",
    "training_data_path = r\"../input/training_set_VU_DM.csv\"\n",
    "test_data_path = r\"../input/test_set_VU_DM.csv\"\n",
    "# df_ori = pd.read_csv(training_data_path)\n",
    "# df_test_ori = pd.read_csv(test_data_path)\n",
    "# df = pd.read_csv(training_data_path).sample(frac=TRAIN_DATA_PERCENTAGE, random_state=42)\n",
    "# df_test = pd.read_csv(test_data_path).sample(frac=TEST_DATA_PERCENTAGE, random_state=42)\n",
    "df = pd.read_csv(training_data_path)\n",
    "df_test = pd.read_csv(test_data_path)\n",
    "# df = df_ori.sample(frac=TRAIN_DATA_PERCENTAGE, random_state=42)\n",
    "# df_test = df_test_ori.sample(frac=TEST_DATA_PERCENTAGE, random_state=42)"
   ],
   "id": "6fa538012884ad37",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:16:30.358705Z",
     "start_time": "2025-05-17T18:16:30.339213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask = (data[column].between(lower_bound, upper_bound)) | (data[column].isna())\n",
    "    data_cleaned = data[mask].copy()\n",
    "    return data_cleaned"
   ],
   "id": "b8dc23a3a14d76ea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:16:32.445927Z",
     "start_time": "2025-05-17T18:16:32.435572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_diff_from_group_mean(data, group_col='srch_id',value_col='prop_starrating'):\n",
    "    data_tmp = data.copy()\n",
    "    mean_col_name = f'{value_col}_mean_by_{group_col}'\n",
    "    data_tmp[mean_col_name] = data_tmp.groupby(group_col)[value_col].transform('mean')\n",
    "\n",
    "    diff_col_name = f'{value_col}_diff_from_mean'\n",
    "    data_tmp[diff_col_name] = data_tmp[value_col] - data_tmp[mean_col_name]\n",
    "    return data_tmp"
   ],
   "id": "1702f02e65ec6c18",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:16:34.576456Z",
     "start_time": "2025-05-17T18:16:34.561746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_book_feature(data):\n",
    "    conditions = [\n",
    "        data['booking_bool'] == 1,\n",
    "        data['click_bool'] == 1\n",
    "    ]\n",
    "\n",
    "    choices = [2, 1]\n",
    "\n",
    "    data['book_feature'] = np.select(conditions, choices, default=0)\n",
    "    data.drop(columns=['booking_bool', 'click_bool'], inplace=True, errors='ignore')\n",
    "    return data"
   ],
   "id": "da138f364640b549",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:49:45.722423Z",
     "start_time": "2025-05-16T11:49:45.701033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_imputation_values(train_data):\n",
    "\n",
    "    # Impute missing values with median or specific values\n",
    "    impute_values = {\n",
    "        \"visitor_hist_starrating\": train_data[\"visitor_hist_starrating\"].median(),\n",
    "        \"visitor_hist_adr_usd\": train_data[\"visitor_hist_adr_usd\"].median(),\n",
    "        \"prop_review_score\": 0,\n",
    "        \"prop_location_score2\": 0,\n",
    "        \"srch_query_affinity_score\": train_data[\"srch_query_affinity_score\"].min(),\n",
    "        \"orig_destination_distance\": train_data[\"orig_destination_distance\"].median()\n",
    "    }\n",
    "\n",
    "    return impute_values\n",
    "\n",
    "def apply_imputation(data, impute_values):\n",
    "    data_impute = data.copy()\n",
    "\n",
    "    data_impute[\"visitor_hist_starrating\"] = data_impute[\"visitor_hist_starrating\"].fillna(impute_values[\"visitor_hist_starrating\"])\n",
    "    data_impute[\"visitor_hist_adr_usd\"] = data_impute[\"visitor_hist_adr_usd\"].fillna(impute_values[\"visitor_hist_adr_usd\"])\n",
    "    data_impute[\"prop_review_score\"] = data_impute[\"prop_review_score\"].fillna(impute_values[\"prop_review_score\"])\n",
    "    data_impute[\"prop_location_score2\"] = data_impute[\"prop_location_score2\"].fillna(impute_values[\"prop_location_score2\"])\n",
    "    data_impute[\"srch_query_affinity_score\"] = data_impute[\"srch_query_affinity_score\"].fillna(impute_values[\"srch_query_affinity_score\"])\n",
    "    data_impute[\"orig_destination_distance\"] = data_impute[\"orig_destination_distance\"].fillna(impute_values[\"orig_destination_distance\"])\n",
    "\n",
    "    logger.debug(\"Imputation finished.\")\n",
    "    return data_impute"
   ],
   "id": "f113b9cc7012b7d3",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:16:38.217740Z",
     "start_time": "2025-05-17T18:16:38.179076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_PRICE_NIGHT = 150000\n",
    "\n",
    "# Generate additional features that may be useful for the model\n",
    "def feature_engineering(data, type='train'):\n",
    "    logger.debug(\"Running feature engineering\")\n",
    "\n",
    "    # Delete rows gross_bookings_usd is significantly different from price_usd\n",
    "    if type=='train':\n",
    "        data['booking_price_diff'] = data['gross_bookings_usd'] - data['price_usd']\n",
    "        data = remove_outliers_iqr(data=data, column='booking_price_diff')\n",
    "        data.drop(columns=[\"booking_price_diff\"], inplace=True)\n",
    "\n",
    "    # transfer date to month and day of week\n",
    "    data['date_time'] = pd.to_datetime(data['date_time'], errors='coerce')\n",
    "    data['month'] = data['date_time'].dt.month\n",
    "    data['dayofweek'] = data['date_time'].dt.dayofweek\n",
    "    data.drop(columns=[\"date_time\"], inplace=True)\n",
    "\n",
    "    # Feature for total number of adults and children\n",
    "    data[\"total_people\"] = data[\"srch_adults_count\"] + data[\"srch_children_count\"]\n",
    "\n",
    "    # History differences\n",
    "    data[\"history_starrating_diff\"] = data[\"visitor_hist_starrating\"] - data[\"prop_starrating\"]\n",
    "\n",
    "    # Total price per night per room\n",
    "    data['price_1room_1night'] = (data['price_usd'] / data['srch_room_count']) / data['srch_length_of_stay']\n",
    "    data[\"history_adr_diff\"] = data[\"visitor_hist_adr_usd\"] - data[\"price_1room_1night\"]\n",
    "\n",
    "    # data.drop(columns=['price_usd'], inplace=True)\n",
    "\n",
    "    # Filter out high prices only for train data\n",
    "    if type=='train':\n",
    "        data = data[data['price_1room_1night'] < MAX_PRICE_NIGHT].copy()\n",
    "\n",
    "    # log transform\n",
    "    data['price_1room_1night_log'] = np.log1p(data['price_1room_1night'])\n",
    "    data[\"price_history_difference\"] = data[\"prop_log_historical_price\"] - data[\"price_1room_1night_log\"]\n",
    "    data['price_1person_1night'] = (data['price_usd'] / data['total_people']) / data['srch_length_of_stay']\n",
    "    data.drop(columns=['price_1room_1night'], inplace=True)\n",
    "\n",
    "    # log transform price, min price is 0, so we use log(x+1)\n",
    "    data['visitor_hist_adr_usd_log'] = np.log1p(data['visitor_hist_adr_usd'])\n",
    "    data.drop(columns=['visitor_hist_adr_usd'], inplace=True)\n",
    "\n",
    "    # Transformations of competitor rates\n",
    "    data[\"avg_comp_rate\"] = data[[\"comp1_rate\", \"comp2_rate\", \"comp3_rate\", \"comp4_rate\", \"comp5_rate\", \"comp6_rate\", \"comp7_rate\", \"comp8_rate\"]].sum(axis=1)\n",
    "    data[\"avg_comp_inv\"] = data[[\"comp1_inv\", \"comp2_inv\", \"comp3_inv\", \"comp4_inv\", \"comp5_inv\", \"comp6_inv\", \"comp7_inv\", \"comp8_inv\"]].sum(axis=1)\n",
    "    data[\"avg_comp_rate_percent_diff\"] = data[[\"comp1_rate_percent_diff\", \"comp2_rate_percent_diff\", \"comp3_rate_percent_diff\", \"comp4_rate_percent_diff\", \"comp5_rate_percent_diff\", \"comp6_rate_percent_diff\", \"comp7_rate_percent_diff\", \"comp8_rate_percent_diff\"]].dropna().median(axis=1)\n",
    "\n",
    "    # Locational features\n",
    "    data[\"domestic_travel_bool\"] = data[\"prop_country_id\"] == data[\"visitor_location_country_id\"]\n",
    "\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_starrating')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='prop_id',value_col='prop_starrating')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='prop_id',value_col='price_1room_1night_log')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_location_score1')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_location_score2')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_destination_id',value_col='price_usd')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_destination_id',value_col='prop_starrating')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='prop_review_score')\n",
    "    data = calculate_diff_from_group_mean(data=data,group_col='srch_id',value_col='promotion_flag')\n",
    "\n",
    "    # # drop original competitor columns\n",
    "    # for x in range(1, 9):\n",
    "    #     data.drop(columns=[f\"comp{x}_rate\", f\"comp{x}_inv\", f\"comp{x}_rate_percent_diff\"], inplace=True, errors='ignore')\n",
    "\n",
    "    data.drop(columns=['srch_destination_id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for c in categorical_feature:\n",
    "        data[c] = data[c].astype('category')\n",
    "\n",
    "    if type=='train':\n",
    "        data = create_book_feature(data)\n",
    "\n",
    "    logger.debug(\"Feature engineering completed\")\n",
    "    return data"
   ],
   "id": "c6e833395965310d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:28:54.902955Z",
     "start_time": "2025-05-17T18:20:58.377719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine imputation values on training set\n",
    "# impute_values = get_imputation_values(df)\n",
    "\n",
    "# Apply the same imputation on training and test\n",
    "# df = apply_imputation(df, impute_values)\n",
    "df = feature_engineering(data=df,type='train')\n",
    "\n",
    "# NOTE ASSIGNMENT PROVIDED TEST SET CONTAINS NO CLICK_BOOL THUS USELESS FOR TESTING\n",
    "# df_test = apply_imputation(df_test, impute_values)\n",
    "df_test = feature_engineering(data=df_test,type='test')\n"
   ],
   "id": "a3d834ec693f14e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:20:58,381 - DEBUG - Running feature engineering\n",
      "2025-05-17 20:25:36,350 - DEBUG - Feature engineering completed\n",
      "2025-05-17 20:25:36,784 - DEBUG - Running feature engineering\n",
      "2025-05-17 20:28:54,753 - DEBUG - Feature engineering completed\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:30:54.791552Z",
     "start_time": "2025-05-17T18:29:39.357888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Split df into training and validation sets based on srch_id ---\n",
    "all_srch_ids = df['srch_id'].unique()\n",
    "train_srch_ids, val_srch_ids = train_test_split(all_srch_ids, test_size=0.05, random_state=42)\n",
    "\n",
    "train_split_df = df[df['srch_id'].isin(train_srch_ids)].copy()\n",
    "val_df = df[df['srch_id'].isin(val_srch_ids)].copy()\n",
    "\n",
    "print(f\"\\nShape of train_split_df: {train_split_df.shape}\")\n",
    "print(f\"Shape of val_df: {val_df.shape}\")\n",
    "print(f\"Number of unique srch_id in train_split_df: {train_split_df['srch_id'].nunique()}\")\n",
    "print(f\"Number of unique srch_id in val_df: {val_df['srch_id'].nunique()}\")\n"
   ],
   "id": "5e07d5b64493293b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of train_split_df: (4698411, 79)\n",
      "Shape of val_df: (247183, 79)\n",
      "Number of unique srch_id in train_split_df: 189797\n",
      "Number of unique srch_id in val_df: 9990\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T18:32:22.630903Z",
     "start_time": "2025-05-17T18:31:45.499936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_cols = [col for col in df.columns if col not in ['srch_id', 'prop_id', 'booking_bool','click_bool', 'position','gross_bookings_usd','book_feature']]\n",
    "\n",
    "label = 'book_feature'\n",
    "# --- Prepare Training Data ---\n",
    "train_split_df = train_split_df.sort_values('srch_id') # Sort by srch_id\n",
    "X_train = train_split_df[feature_cols]\n",
    "y_train = train_split_df[label]\n",
    "group_train = train_split_df.groupby('srch_id', sort=False).size().to_list()\n",
    "\n",
    "print(f\"\\nNumber of training groups (searches) for actual training: {len(group_train)}\")\n",
    "print(f\"Total training samples for actual training: {sum(group_train)}\")\n",
    "\n",
    "# --- Prepare Validation Data ---\n",
    "val_df = val_df.sort_values('srch_id') # Sort by srch_id\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[label]\n",
    "group_val = val_df.groupby('srch_id', sort=False).size().to_list()\n",
    "\n",
    "print(f\"\\nNumber of validation groups (searches): {len(group_val)}\")\n",
    "print(f\"Total validation samples: {sum(group_val)}\")\n",
    "\n",
    "# --- Prepare Test Data  ---\n",
    "X_test = df_test[feature_cols]\n",
    "# We need srch_id and prop_id from test_df for final output generation\n",
    "test_ids = df_test[['srch_id', 'prop_id']].copy()\n",
    "len(X_test)"
   ],
   "id": "785aa8a587dd2daf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training groups (searches) for actual training: 189797\n",
      "Total training samples for actual training: 4698411\n",
      "\n",
      "Number of validation groups (searches): 9990\n",
      "Total validation samples: 247183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4959183"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:33:58.700335Z",
     "start_time": "2025-05-17T18:36:48.299366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Train LGBMRanker Model\n",
    "print(\"\\nTraining LGBMRanker model...\")\n",
    "\n",
    "ranker = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\",  # Core objective for learning to rank\n",
    "    metric=\"ndcg\",           # Evaluation metric (Normalized Discounted Cumulative Gain)\n",
    "    n_estimators=1000,        # Number of boosting rounds\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,           # No limit on depth\n",
    "    num_leaves=62,          # Number of leaves in each tree\n",
    "    importance_type='gain',\n",
    "    label_gain=[0, 1, 5],      # The gain for unbooked (0) is 0, click (1) is 1,book(2) is 5\n",
    "    random_state=42,\n",
    "    n_jobs=-1,     # Use all available cores\n",
    "    boosting='dart'\n",
    "    # Add other parameters as needed, e.g., num_leaves, max_depth, reg_alpha, reg_lambda\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    group=group_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)], # Use validation set here\n",
    "    eval_group=[group_train, group_val],          # Group info for validation set\n",
    "    eval_names=['train', 'valid'],                # Names for the eval sets\n",
    "    eval_at=[5, 10],                              # Evaluate NDCG@k\n",
    "    callbacks=[lgb.early_stopping(200)] # Adjusted early stopping\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "print(\"\\nFeature Importances:\")\n",
    "feature_importances = pd.Series(ranker.feature_importances_, index=feature_cols)\n",
    "print(feature_importances.sort_values(ascending=False))"
   ],
   "id": "74978c2d5daa40c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LGBMRanker model...\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.297400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8897\n",
      "[LightGBM] [Info] Number of data points in the train set: 4698411, number of used features: 74\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiajian/Projects/DataMining/.venv/lib/python3.11/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "Feature Importances:\n",
      "prop_location_score2               797059.637478\n",
      "prop_country_id                    714168.172316\n",
      "visitor_location_country_id        493537.204455\n",
      "price_usd                          431440.608842\n",
      "price_usd_diff_from_mean           250338.669294\n",
      "                                       ...      \n",
      "comp1_inv                             134.965998\n",
      "comp6_inv                              88.211399\n",
      "comp4_inv                              69.160501\n",
      "prop_starrating_mean_by_prop_id         0.000000\n",
      "avg_comp_rate_percent_diff              0.000000\n",
      "Length: 74, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:42:16.060710Z",
     "start_time": "2025-05-17T20:34:26.991775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Make Predictions on Test Data\n",
    "print(\"\\nPredicting on test data...\")\n",
    "# a NumPy array of scores. Higher scores indicate a higher predicted likelihood of relevance (booking)\n",
    "test_predictions = ranker.predict(X_test)\n",
    "# test_ids = ['srch_id', 'prop_id', 'predicted_score']\n",
    "test_ids['predicted_score'] = test_predictions"
   ],
   "id": "875e8c70208bd0b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on test data...\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:51:42.232063Z",
     "start_time": "2025-05-17T20:51:42.158823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(test_predictions)\n",
    "feature_importances"
   ],
   "id": "78393f2e1919b2b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id                                         35072.116268\n",
       "visitor_location_country_id                    493537.204455\n",
       "visitor_hist_starrating                          5171.114796\n",
       "prop_country_id                                714168.172316\n",
       "prop_starrating                                125823.941402\n",
       "                                                   ...      \n",
       "prop_starrating_mean_by_srch_destination_id     10155.255823\n",
       "prop_review_score_mean_by_srch_id               10505.272598\n",
       "prop_review_score_diff_from_mean                68257.577362\n",
       "promotion_flag_mean_by_srch_id                   6719.228981\n",
       "promotion_flag_diff_from_mean                  152492.430017\n",
       "Length: 74, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:43:05.195901Z",
     "start_time": "2025-05-17T20:42:48.729756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Rank Properties and Generate Output File\n",
    "print(\"Ranking properties and generating output file...\")\n",
    "\n",
    "# Sort properties within each search_id group by the predicted score\n",
    "test_ids_sorted = test_ids.sort_values(['srch_id', 'predicted_score'], ascending=[True, False])\n",
    "\n",
    "# Select only srch_id and prop_id for the final output\n",
    "output_df = test_ids_sorted[['srch_id', 'prop_id']]\n",
    "\n",
    "output_filename = \"LGBMRanker_predict.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nOutput file '{output_filename}' generated successfully.\")\n",
    "print(\"Sample of the output file:\")\n",
    "print(output_df.head(10))"
   ],
   "id": "7cb81150acecd010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking properties and generating output file...\n",
      "\n",
      "Output file 'LGBMRanker_predict.csv' generated successfully.\n",
      "Sample of the output file:\n",
      "    srch_id  prop_id\n",
      "23        1    99484\n",
      "12        1    61934\n",
      "9         1    54937\n",
      "5         1    28181\n",
      "4         1    24194\n",
      "8         1    50162\n",
      "6         1    34263\n",
      "22        1    95031\n",
      "20        1    90385\n",
      "13        1    63894\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:51:08.382285Z",
     "start_time": "2025-05-17T20:51:07.977957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save model\n",
    "model_filename_lgb = r'model/lgbm_ranker_model.txt'\n",
    "ranker.booster_.save_model(model_filename_lgb)\n",
    "print(f\"save model as LightGBM format: {model_filename_lgb}\")\n",
    "\n",
    "# load model\n",
    "# loaded_ranker_lgb = lgb.Booster(model_file='lgbm_ranker_model.txt')\n",
    "# ranker_sklearn_loaded = lgb.LGBMRanker() # new ranker instance\n",
    "# ranker_sklearn_loaded.booster_ = loaded_ranker_lgb #\n",
    "\n"
   ],
   "id": "2a978ed5f452cddb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model as LightGBM format: model/lgbm_ranker_model.txt\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
