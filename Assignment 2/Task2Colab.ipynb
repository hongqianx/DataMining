{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8e217",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import optuna\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import datetime as dt\n",
    "from cuml.ensemble import RandomForestRegressor as cuMLRandomForestRegressor\n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from cuml.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from cuml.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# --- GPU Check ---\n",
    "try:\n",
    "    HAS_GPU = cp.cuda.runtime.getDeviceCount() > 0\n",
    "    if not HAS_GPU:\n",
    "        raise SystemExit(\"No CUDA for GPU found. This script requires a GPU\")\n",
    "    cp.cuda.Device(0).use()\n",
    "    print(f\"Found {cp.cuda.runtime.getDeviceCount()} GPUs. Using device 0\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"GPU check or cupy import failed: {e}\")\n",
    "\n",
    "# Set up a basic logger\n",
    "logger = logging.getLogger(\"MLLogger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# --- Configuration ---\n",
    "training_data_path = r\"../input/training_set_VU_DM.csv\"\n",
    "test_data_path = r\"../input/test_set_VU_DM.csv\"\n",
    "FOLD_AMOUNT = 3\n",
    "TESTSPLIT_RATIO = 10\n",
    "OPTUNA_TRIALS = 1\n",
    "ENSEMBLE_N_ESTIMATORS = 10 # Estimators for final ensemble model\n",
    "TRAIN_WITHOUT_EVALUATION = False\n",
    "RANDOM_STATE = 42\n",
    "DATA_PRECISION = cp.float32 # For GPU handling\n",
    "\n",
    "# --- Data Loading (GPU) ---\n",
    "logger.info(\"Loading data directly into DataFrames...\")\n",
    "try:\n",
    "    df = cudf.read_csv(training_data_path)\n",
    "    df_test = cudf.read_csv(test_data_path)\n",
    "    logger.info(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Failed to load data: {e}\")\n",
    "\n",
    "# --- Start preprocessing ---\n",
    "def feature_engineering(data):\n",
    "    logger.debug(\"Running feature engineering\")\n",
    "    data_fe = data.copy()\n",
    "\n",
    "    # Feature for total number of persons\n",
    "    data_fe[\"total_people\"] = data_fe[\"srch_adults_count\"] + data_fe[\"srch_children_count\"]\n",
    "\n",
    "    # Total price per night\n",
    "    data_fe[\"srch_length_of_stay_safe\"] = data_fe[\"srch_length_of_stay\"].replace(0, 1) \n",
    "    data_fe[\"price_per_night\"] = data_fe[\"price_usd\"] / data_fe[\"srch_length_of_stay_safe\"]\n",
    "    data_fe = data_fe.drop(columns=[\"srch_length_of_stay_safe\"])\n",
    "    data_fe[\"price_per_night\"] = data_fe[\"price_per_night\"].fillna(0)\n",
    "\n",
    "    # History differences\n",
    "    data_fe[\"history_starrating_diff\"] = data_fe[\"visitor_hist_starrating\"] - data_fe[\"prop_starrating\"]\n",
    "    data_fe[\"history_adr_diff\"] = data_fe[\"visitor_hist_adr_usd\"] - data_fe[\"price_usd\"]\n",
    "\n",
    "    # Competitor features\n",
    "    comp_cols_base = [\"comp1\", \"comp2\", \"comp3\", \"comp4\", \"comp5\", \"comp6\", \"comp7\", \"comp8\"]\n",
    "    comp_rate_cols = [f\"{c}_rate\" for c in comp_cols_base if f\"{c}_rate\" in data_fe.columns]\n",
    "    comp_inv_cols = [f\"{c}_inv\" for c in comp_cols_base if f\"{c}_inv\" in data_fe.columns]\n",
    "    comp_rate_diff_cols = [f\"{c}_rate_percent_diff\" for c in comp_cols_base if f\"{c}_rate_percent_diff\" in data_fe.columns]\n",
    "\n",
    "    # Transformations of competitor rates\n",
    "    data_fe[\"avg_comp_rate\"] = data_fe[comp_rate_cols].mean(axis=1).fillna(0)\n",
    "    data_fe[\"avg_comp_inv\"] = data_fe[comp_inv_cols].mean(axis=1).fillna(0)\n",
    "    data_fe[\"avg_comp_rate_percent_diff\"] = data_fe[comp_rate_diff_cols].mean(axis=1).fillna(0)\n",
    "    \n",
    "    # Locational features\n",
    "    data_fe[\"customer_hotel_country_equal\"] = (data_fe[\"prop_country_id\"] == data_fe[\"visitor_location_country_id\"])\n",
    "\n",
    "    # Convert boolean features generated to int8\n",
    "    for col in data_fe.select_dtypes(include=['bool']).columns:\n",
    "         data_fe[col] = data_fe[col].astype(cp.int8)\n",
    "\n",
    "    logger.debug(\"Feature engineering finished\")\n",
    "    return data_fe\n",
    "\n",
    "\n",
    "def get_imputation_values_cudf(train_data):\n",
    "    logger.debug(\"Calculating imputation values\")\n",
    "    impute_values = {}\n",
    "    impute_values[\"visitor_hist_starrating\"] = train_data[\"visitor_hist_starrating\"].dropna().median()\n",
    "    impute_values[\"visitor_hist_adr_usd\"] = train_data[\"visitor_hist_adr_usd\"].dropna().median()\n",
    "    impute_values[\"prop_review_score\"] = 0.0\n",
    "    impute_values[\"prop_location_score2\"] = 0.0\n",
    "    impute_values[\"srch_query_affinity_score\"] = train_data[\"srch_query_affinity_score\"].dropna().min() \n",
    "    impute_values[\"orig_destination_distance\"] = train_data[\"orig_destination_distance\"].dropna().median() \n",
    "    impute_values[\"price_usd_cap\"] = 20000.0\n",
    "    impute_values[\"price_usd_median\"] = train_data[\"price_usd\"].dropna().median()\n",
    "\n",
    "    for x in range(1, 9):\n",
    "        for suffix in [\"rate\", \"inv\", \"rate_percent_diff\"]:\n",
    "            impute_key = f\"comp{x}_{suffix}\"\n",
    "            impute_values[impute_key] = 0.0\n",
    "            try:\n",
    "                numeric_col = cudf.to_numeric(train_data[impute_key])\n",
    "                median_val = float(numeric_col.dropna().median())\n",
    "                if not np.isnan(median_val):\n",
    "                    impute_values[impute_key] = median_val\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not calculate median for {impute_key}: {e}. Using 0\")\n",
    "\n",
    "    # Ensure all impute values are float\n",
    "    for k, v in impute_values.items():\n",
    "        if pd.isna(v):\n",
    "            logger.warning(f\"Imputation value for {k} is NaN, setting to 0.\")\n",
    "            impute_values[k] = 0.0\n",
    "    logger.debug(\"Imputation values calculated.\")\n",
    "    return impute_values\n",
    "\n",
    "def apply_imputation_cudf(data, impute_values):\n",
    "    logger.debug(\"Applying imputation\")\n",
    "    df = data.copy()\n",
    "\n",
    "    # Apply pre-calculated imputation values\n",
    "    df[\"visitor_hist_starrating\"] = df[\"visitor_hist_starrating\"].fillna(impute_values[\"visitor_hist_starrating\"])\n",
    "    df[\"visitor_hist_adr_usd\"] = df[\"visitor_hist_adr_usd\"].fillna(impute_values[\"visitor_hist_adr_usd\"])\n",
    "    df[\"prop_review_score\"] = df[\"prop_review_score\"].fillna(impute_values[\"prop_review_score\"])\n",
    "    df[\"prop_location_score2\"] = df[\"prop_location_score2\"].fillna(impute_values[\"prop_location_score2\"])\n",
    "    df[\"srch_query_affinity_score\"] = df[\"srch_query_affinity_score\"].fillna(impute_values[\"srch_query_affinity_score\"])\n",
    "    df[\"orig_destination_distance\"] = df[\"orig_destination_distance\"].fillna(impute_values[\"orig_destination_distance\"])\n",
    "\n",
    "    for x in range(1, 9):\n",
    "         for suffix in [\"rate\", \"inv\", \"rate_percent_diff\"]:\n",
    "             col = f\"comp{x}_{suffix}\"\n",
    "             impute_key = f\"comp{x}_{suffix}\"\n",
    "             df[col] = df[col].fillna(impute_values[impute_key])\n",
    "\n",
    "    df[\"price_usd\"] = df[\"price_usd\"].fillna(impute_values[\"price_usd_median\"])\n",
    "    df[\"price_usd\"] = df[\"price_usd\"].clip(upper=impute_values[\"price_usd_cap\"])\n",
    "\n",
    "    logger.debug(\"Imputation finished.\")\n",
    "    return df\n",
    "\n",
    "def transform_data_cudf(data):\n",
    "    logger.debug(\"Transforming datetime\")\n",
    "    dt_col = cudf.to_datetime(data['date_time'])\n",
    "    data['date_time_epoch'] = (dt_col.astype('int64') // 10**9) # Convert to epoch\n",
    "    data = data.drop(columns=['date_time'])\n",
    "    logger.debug(\"Datetime transformation finished.\")\n",
    "    return data\n",
    "\n",
    "# --- Preprocessing Pipeline Execution (GPU) ---\n",
    "logger.info(\"Starting preprocessing pipeline (GPU)...\")\n",
    "\n",
    "impute_values = get_imputation_values_cudf(df)\n",
    "\n",
    "df = transform_data_cudf(df)\n",
    "df = apply_imputation_cudf(df, impute_values)\n",
    "df = feature_engineering(df)\n",
    "\n",
    "df_test_ids = df_test[['srch_id', 'prop_id']].copy()\n",
    "df_test = transform_data_cudf(df_test)\n",
    "df_test = apply_imputation_cudf(df_test, impute_values)\n",
    "df_test = feature_engineering(df_test)\n",
    "\n",
    "logger.info(\"Preprocessing pipeline finished.\")\n",
    "\n",
    "# --- Target and Feature Selection ---\n",
    "target_value = \"click_bool\"\n",
    "exclude_values = [\"booking_bool\", \"position\", \"gross_bookings_usd\", \"srch_id\", \"prop_id\"]\n",
    "exclude_values_filtered = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in exclude_values]\n",
    "exclude_values_for_X = [target_value] + exclude_values_filtered\n",
    "\n",
    "feature_cols_final = [col for col in df.columns if col in df_test.columns and col not in exclude_values_for_X]\n",
    "\n",
    "X = df[feature_cols_final].copy()\n",
    "y = df[target_value].astype(DATA_PRECISION).copy()\n",
    "X_kaggle_test = df_test[feature_cols_final].copy()\n",
    "\n",
    "# Added because there were some memory issues.\n",
    "del df\n",
    "del df_test\n",
    "logger.info(\"Original Dataframe deleted\")\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "# --- Train/Validation split ---\n",
    "if TRAIN_WITHOUT_EVALUATION:\n",
    "    logger.info(\"Training on full dataset. No validation split.\")\n",
    "    x_train = X\n",
    "    y_train = y\n",
    "    x_val = None\n",
    "    y_val = None\n",
    "else:\n",
    "    logger.info(f\"Splitting data using {TESTSPLIT_RATIO}% for validation.\")\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=TESTSPLIT_RATIO/100, random_state=RANDOM_STATE)\n",
    "    logger.info(f\"Train shape: {x_train.shape}, Validation shape: {x_val.shape}\")\n",
    "    del X, y\n",
    "\n",
    "kf = KFold(n_splits=FOLD_AMOUNT, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- Hyperparameter Optimization (GPU) ---\n",
    "def hyperOptimization(trial, model_name):\n",
    "    params = {}\n",
    "    num_boost_round = 100\n",
    "    if model_name == 'xgb':\n",
    "        num_boost_round = trial.suggest_int('xgb_n_estimators', 100, 400)\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',      \n",
    "            'eta': trial.suggest_float('xgb_learning_rate', 1e-3, 0.2, log=True),\n",
    "            'max_depth': trial.suggest_int('xgb_max_depth', 4, 10),\n",
    "            'subsample': trial.suggest_float('xgb_subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0),\n",
    "            'tree_method': 'hist', # Use hist method for GPU\n",
    "            'device': 'cuda',\n",
    "            'verbosity': 0,\n",
    "            'random_state': RANDOM_STATE\n",
    "        }\n",
    "    elif model_name == 'lgbm':\n",
    "        num_boost_round = trial.suggest_int('lgbm_n_estimators', 100, 400)\n",
    "        params = {\n",
    "            'objective': 'regression_l2', \n",
    "            'metric': 'rmse',           \n",
    "            'learning_rate': trial.suggest_float('lgbm_learning_rate', 1e-3, 0.2, log=True),\n",
    "            'num_leaves': trial.suggest_int('lgbm_num_leaves', 20, 80),\n",
    "            'max_depth': trial.suggest_int('lgbm_max_depth', 4, 10),\n",
    "            'subsample': trial.suggest_float('lgbm_subsample', 0.6, 1.0), \n",
    "            'colsample_bytree': trial.suggest_float('lgbm_colsample_bytree', 0.6, 1.0), \n",
    "            'device_type': 'GPU', #Use Cuda if can\n",
    "            'verbosity': -1,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': 1 # Somewhere recommended to set to 1 for GPU, so just use that for now\n",
    "        }\n",
    "    elif model_name == 'rf':\n",
    "         params = {\n",
    "             'n_estimators': trial.suggest_int('rf_n_estimators', 50, 200),\n",
    "             'max_depth': trial.suggest_int('rf_max_depth', 5, 16),\n",
    "             'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 10),\n",
    "             'n_streams': 1\n",
    "         }\n",
    "    elif model_name == 'catboost':\n",
    "        iterations = trial.suggest_int('catboost_iterations', 100, 400)\n",
    "        params = {\n",
    "            'loss_function': 'RMSE',    \n",
    "            'learning_rate': trial.suggest_float('catboost_learning_rate', 1e-3, 0.2, log=True),\n",
    "            'depth': trial.suggest_int('catboost_depth', 4, 10),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'verbose': 0,                \n",
    "            'task_type': 'GPU',          # Use GPU\n",
    "            'devices': '0'               \n",
    "        }\n",
    "        params['iterations'] = iterations\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "    cv_rmse = []\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
    "        logger.debug(f\"Training {model_name} fold {fold_idx + 1}/{FOLD_AMOUNT} for Optuna trial {trial.number}\")\n",
    "        train_idx_list = train_index.tolist()\n",
    "        val_idx_list = val_index.tolist()\n",
    "\n",
    "        X_train_cv = x_train.iloc[train_idx_list]\n",
    "        X_val_cv = x_train.iloc[val_idx_list]\n",
    "        y_train_cv = y_train.iloc[train_idx_list]\n",
    "        y_val_cv = y_train.iloc[val_idx_list]\n",
    "\n",
    "        model = None\n",
    "        y_pred = None\n",
    "\n",
    "        start_time = dt.datetime.now()\n",
    "        try:\n",
    "            if model_name == 'lgbm':\n",
    "                # Use cupy arrays for lgb dataset\n",
    "                X_train_cp = X_train_cv\n",
    "                y_train_cp = y_train_cv\n",
    "                X_val_cp = X_val_cv\n",
    "\n",
    "                lgb_train_data = lgb.Dataset(X_train_cp, label=y_train_cp)\n",
    "                model = lgb.train(params, lgb_train_data, num_boost_round=num_boost_round)\n",
    "                y_pred = model.predict(X_val_cp) \n",
    "                del X_train_cp, y_train_cp, X_val_cp \n",
    "\n",
    "            elif model_name == 'xgb':\n",
    "                # Use cuDF directly for DMatrix\n",
    "                dtrain = xgb.DMatrix(X_train_cv, label=y_train_cv)\n",
    "                dval = xgb.DMatrix(X_val_cv)\n",
    "                model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "                y_pred = model.predict(dval)\n",
    "\n",
    "            elif model_name == 'catboost':\n",
    "                train_pool = cb.Pool(data=X_train_cv, label=y_train_cv)\n",
    "                model = cb.CatBoostRegressor(**params)\n",
    "                model.fit(train_pool)\n",
    "                y_pred = model.predict(X_val_cv) \n",
    "\n",
    "            elif model_name == 'rf':\n",
    "                model = cuMLRandomForestRegressor(**params, random_state=RANDOM_STATE)\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                y_pred = model.predict(X_val_cv)\n",
    "\n",
    "            else:\n",
    "                 raise ValueError(f\"Training logic missing for {model_name}\")\n",
    "\n",
    "            end_time = dt.datetime.now()\n",
    "            logger.debug(f\"Fold {fold_idx+1} fit duration: {end_time - start_time}\")\n",
    "\n",
    "            y_pred_cp = cp.asarray(y_pred)\n",
    "            y_val_cp = y_val_cv.values if isinstance(y_val_cv, cudf.Series) else cp.asarray(y_val_cv)\n",
    "\n",
    "            rmse = cp.sqrt(cp.mean((y_pred_cp - y_val_cp)**2))\n",
    "            cv_rmse.append(rmse.item())\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training/prediction for {model_name} trial {trial.number} fold {fold_idx+1}: {e}\", exc_info=True)\n",
    "            # Return a high value to Optuna if a fold fails\n",
    "            return float('inf')\n",
    "        finally:\n",
    "            # Clean up fold data\n",
    "             try: \n",
    "                del X_train_cv\n",
    "                del X_val_cv\n",
    "                del y_train_cv\n",
    "                del y_val_cv\n",
    "                del y_pred\n",
    "                del y_pred_cp\n",
    "                del y_val_cp\n",
    "                del model\n",
    "             except Exception: \n",
    "                 logger.error(\"Error during cleanup of fold data\")\n",
    "             cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    avg_rmse = np.mean(cv_rmse)\n",
    "    logger.info(f\"Model: {model_name}, Trial: {trial.number}, Avg CV RMSE: {avg_rmse:.5f}\")\n",
    "    return avg_rmse\n",
    "\n",
    "# --- Main Training Loop\n",
    "models_to_optimize = ['xgb', 'lgbm', 'rf', 'catboost']\n",
    "best_trained_models = []\n",
    "best_params_dict = {} \n",
    "\n",
    "for model_name in models_to_optimize:\n",
    "    logger.info(f\"--- Optimizing {model_name} ---\")\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps=3, n_startup_trials=3)\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "    logger.info(f\"Starting Optuna study for {model_name} with {OPTUNA_TRIALS} trials.\")\n",
    "\n",
    "    try:\n",
    "        study.optimize(lambda trial: hyperOptimization(trial, model_name), n_trials=OPTUNA_TRIALS)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Optimization failed for {model_name}: {e}\", exc_info=True)\n",
    "        continue # Skip to next model if optimization fails\n",
    "\n",
    "    logger.info(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n",
    "    logger.info(f\"Best CV RMSE for {model_name}: {study.best_value:.5f}\")\n",
    "    best_params_dict[model_name] = study.best_params\n",
    "\n",
    "    logger.info(f\"Training final {model_name} with best hyperparameters on full training data\")\n",
    "    start_time = dt.datetime.now()\n",
    "    final_model = None\n",
    "    try:\n",
    "        prefix = model_name + \"_\"\n",
    "        current_best_params = {\n",
    "            key.replace(prefix, ''): value\n",
    "            for key, value in study.best_params.items()\n",
    "        }\n",
    "\n",
    "        if model_name == 'xgb':\n",
    "            params = {'objective': 'reg:squarederror', 'tree_method': 'hist', 'device': 'cuda', 'verbosity': 0, 'random_state': RANDOM_STATE}\n",
    "            params.update(current_best_params) # Add tuned params\n",
    "            num_boost_round = params.pop('n_estimators', 100) # Get n_estimators, remove from params dict for xgb.train\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            final_model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "        elif model_name == 'lgbm':\n",
    "            params = {'objective': 'regression_l2', 'metric': 'rmse', 'device_type': 'gpu', 'verbosity': -1, 'random_state': RANDOM_STATE, 'n_jobs': 1}\n",
    "            params.update(current_best_params)\n",
    "            num_boost_round = params.pop('n_estimators', 100)\n",
    "\n",
    "            x_train_cp = x_train.to_cupy()\n",
    "            y_train_cp = y_train.to_cupy()\n",
    "            lgb_train_data = lgb.Dataset(data=x_train_cp, label=y_train_cp)\n",
    "\n",
    "            # TODO Cuda not working for lgbm\n",
    "            final_model = lgb.train(params, lgb_train_data, num_boost_round=num_boost_round)\n",
    "        elif model_name == 'catboost':\n",
    "            params = {'loss_function': 'RMSE', 'verbose': 0, 'random_state': RANDOM_STATE, 'task_type': 'GPU', 'devices': '0'}\n",
    "            params.update(current_best_params)\n",
    "            iterations = params.pop('iterations', 100) \n",
    "            train_pool = cb.Pool(data=x_train, label=y_train)\n",
    "            model_instance = cb.CatBoostRegressor(**params, iterations=iterations) \n",
    "            model_instance.fit(train_pool)\n",
    "            final_model = model_instance\n",
    "        elif model_name == 'rf':\n",
    "             params = {'n_streams': 1, 'random_state': RANDOM_STATE} \n",
    "             params.update(current_best_params)\n",
    "             model_instance = cuMLRandomForestRegressor(**params)\n",
    "             model_instance.fit(x_train, y_train)\n",
    "             final_model = model_instance\n",
    "\n",
    "        end_time = dt.datetime.now()\n",
    "        logger.info(f\"Final {model_name} trained in {end_time - start_time}.\")\n",
    "        if final_model:\n",
    "            best_trained_models.append((model_name, final_model))\n",
    "        else:\n",
    "            logger.error(f\"Final model training failed for {model_name}, model object is None.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during final model training for {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "    # Clear memory\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# --- Stacking Ensemble (Manual GPU) ---\n",
    "if not best_trained_models:\n",
    "    raise SystemExit(\"No base models were trained successfully. Cannot proceed with stacking\")\n",
    "\n",
    "logger.info(f\"--- Building ensemble with {len(best_trained_models)} base models ---\")\n",
    "\n",
    "meta_features_train_list = []\n",
    "\n",
    "for name, model in best_trained_models:\n",
    "    logger.debug(f\"Predicting with base model: {name} on training data\")\n",
    "    y_pred = None\n",
    "    try:\n",
    "        if name == 'lgbm':\n",
    "            y_pred = model.predict(x_train.to_cupy())\n",
    "        elif name == 'xgb':\n",
    "            dtrain_pred = xgb.DMatrix(x_train)\n",
    "            y_pred = model.predict(dtrain_pred)\n",
    "        elif name == 'catboost':\n",
    "            y_pred = model.predict(x_train)\n",
    "        elif name == 'rf':\n",
    "            y_pred = model.predict(x_train)\n",
    "\n",
    "        preds_cp = cp.asarray(y_pred)\n",
    "        meta_features_train_list.append(preds_cp)\n",
    "        del y_pred, preds_cp\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get predictions from base model {name} on train data: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Failed prediction for stacking from model {name}\") from e\n",
    "\n",
    "meta_features_train = cp.column_stack(meta_features_train_list)\n",
    "del meta_features_train_list\n",
    "logger.info(f\"Training meta-features shape: {meta_features_train.shape}\")\n",
    "\n",
    "# Define and train the final ensemble\n",
    "ensemble_model = cuMLRandomForestRegressor(\n",
    "    n_estimators=ENSEMBLE_N_ESTIMATORS,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_streams = 2 \n",
    "    )\n",
    "\n",
    "logger.info(\"Training final estimator using combined models\")\n",
    "start_time = dt.datetime.now()\n",
    "ensemble_model.fit(meta_features_train, y_train_cp)\n",
    "end_time = dt.datetime.now()\n",
    "logger.info(f\"Final estimator trained in {end_time - start_time}.\")\n",
    "\n",
    "del x_train, y_train, y_train_cp, meta_features_train\n",
    "logger.info(\"Deleted training data and training meta-features.\")\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# --- Evaluation (if validation set exists) ---\n",
    "if not TRAIN_WITHOUT_EVALUATION and x_val is not None and y_val is not None:\n",
    "    logger.info(\"--- Evaluating Stacking Ensemble on Validation Set ---\")\n",
    "    logger.info(\"Generating meta-features on validation data...\")\n",
    "    meta_features_val_list = []\n",
    "    y_val_cp = y_val.values if isinstance(y_val, cudf.Series) else cp.asarray(y_val)\n",
    "\n",
    "    for name, model in best_trained_models:\n",
    "        logger.debug(f\"Predicting with base model: {name} on validation data\")\n",
    "        y_pred = None\n",
    "        try:\n",
    "            if name == 'lgbm':\n",
    "                y_pred = model.predict(x_val.to_cupy())\n",
    "            elif name == 'xgb':\n",
    "                dval_pred = xgb.DMatrix(x_val)\n",
    "                y_pred = model.predict(dval_pred)\n",
    "            elif name == 'catboost':\n",
    "                y_pred = model.predict(x_val)\n",
    "            elif name == 'rf':\n",
    "                y_pred = model.predict(x_val)\n",
    "\n",
    "            preds_cp = cp.asarray(y_pred)\n",
    "            meta_features_val_list.append(preds_cp)\n",
    "            del y_pred, preds_cp\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Failed to get predictions from base model {name} on validation data: {e}\", exc_info=True)\n",
    "             raise RuntimeError(f\"Failed prediction for stacking evaluation from model {name}\") from e\n",
    "\n",
    "    meta_features_val = cp.column_stack(meta_features_val_list)\n",
    "    del meta_features_val_list\n",
    "    logger.info(f\"Validation meta-features shape: {meta_features_val.shape}\")\n",
    "\n",
    "    logger.info(\"Predicting with final estimator on validation meta-features...\")\n",
    "    stacking_predictions_val = ensemble_model.predict(meta_features_val)\n",
    "\n",
    "    stacking_rmse = cp.sqrt(mean_squared_error(y_val_cp, stacking_predictions_val))\n",
    "    stacking_mae = mean_absolute_error(y_val_cp, stacking_predictions_val)\n",
    "    stacking_r2 = r2_score(y_val_cp, stacking_predictions_val)\n",
    "\n",
    "    logger.info(f\"Validation RMSE: {stacking_rmse}\")\n",
    "    logger.info(f\"Validation MAE: {stacking_mae}\")\n",
    "    logger.info(f\"Validation R²: {stacking_r2}\")\n",
    "\n",
    "    try:\n",
    "        y_val_np = cp.asnumpy(y_val_cp)\n",
    "        y_pred_class_np = cp.asnumpy(stacking_predictions_val >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_val_np, y_pred_class_np)\n",
    "        logger.info(f\"Validation Accuracy (threshold 0.5): {acc*100:.2f}%\")\n",
    "        del y_val_np, y_pred_class_np\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not calculate accuracy: {e}\")\n",
    "\n",
    "    del x_val, y_val, y_val_cp, meta_features_val, stacking_predictions_val\n",
    "    logger.info(\"Deleted validation data and validation meta-features.\")\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "else:\n",
    "     logger.info(\"Skipping validation evaluation.\")\n",
    "\n",
    "# --- Kaggle Submission ---\n",
    "logger.info(\"--- Generating Kaggle Submission ---\")\n",
    "logger.info(\"Generating meta-features on Kaggle test data (X_kaggle_test)...\")\n",
    "meta_features_kaggle_list = []\n",
    "\n",
    "# Ensure X_kaggle_test has the correct columns before prediction\n",
    "X_kaggle_test = X_kaggle_test[feature_cols_final]\n",
    "\n",
    "for name, model in best_trained_models:\n",
    "    logger.debug(f\"Predicting with base model: {name} on Kaggle test data\")\n",
    "    y_pred = None\n",
    "    try:\n",
    "        if name == 'lgbm':\n",
    "            y_pred = model.predict(X_kaggle_test.to_cupy())\n",
    "        elif name == 'xgb':\n",
    "            dkaggle_pred = xgb.DMatrix(X_kaggle_test)\n",
    "            y_pred = model.predict(dkaggle_pred)\n",
    "        elif name == 'catboost':\n",
    "            y_pred = model.predict(X_kaggle_test)\n",
    "        elif name == 'rf':\n",
    "            y_pred = model.predict(X_kaggle_test)\n",
    "\n",
    "        preds_cp = cp.asarray(y_pred)\n",
    "        meta_features_kaggle_list.append(preds_cp)\n",
    "        del y_pred, preds_cp\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get predictions from base model {name} on Kaggle test data: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Failed prediction for Kaggle submission from model {name}\") from e\n",
    "\n",
    "meta_features_kaggle = cp.column_stack(meta_features_kaggle_list)\n",
    "del meta_features_kaggle_list\n",
    "\n",
    "logger.info(\"Predicting with final ensemble on Kaggle test dataset\")\n",
    "kaggle_predictions_gpu = ensemble_model.predict(meta_features_kaggle)\n",
    "\n",
    "kaggle_predictions_np = cp.asnumpy(kaggle_predictions_gpu)\n",
    "logger.info(\"Kaggle predictions generated\")\n",
    "\n",
    "del X_kaggle_test, meta_features_kaggle, kaggle_predictions_gpu\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# --- Submission File Creation ---\n",
    "def create_submission_file(test_identifiers_cudf, predictions_numpy, output_filename=\"submission.csv\"):\n",
    "    logger.info(f\"Creating submission file: {output_filename}\")\n",
    "    # Ensure correct format\n",
    "    submission_df = test_identifiers_cudf[['srch_id', 'prop_id']].reset_index(drop=True)\n",
    "\n",
    "    if len(submission_df) != len(predictions_numpy):\n",
    "        raise ValueError(f\"Length mismatch: Identifiers ({len(submission_df)}) vs Predictions ({len(predictions_numpy)})\")\n",
    "\n",
    "    pred_series = cudf.Series(predictions_numpy, name='prediction_score')\n",
    "    submission_df['prediction_score'] = pred_series.astype(DATA_PRECISION)\n",
    "\n",
    "    logger.info(\"Sorting submission data on GPU...\")\n",
    "    submission_sorted = submission_df.sort_values(\n",
    "        by=['srch_id', 'prediction_score'],\n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # Select final columns required for submission\n",
    "    final_submission_gpu = submission_sorted[['srch_id', 'prop_id']]\n",
    "\n",
    "    logger.info(\"Converting final submission to DataFrame\")\n",
    "    try:\n",
    "        final_submission_pd = final_submission_gpu.to_pandas()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to convert final submission GPU to Pandas: {e}.\")\n",
    "        try:\n",
    "            final_submission_gpu.to_csv(output_filename, index=False)\n",
    "            logger.info(f\"Kaggle submission file saved to: {output_filename}\")\n",
    "            return\n",
    "        except Exception as e_cudf_save:\n",
    "            logger.error(f\"Direct save failed: {e_cudf_save}\")\n",
    "\n",
    "    logger.info(f\"Writing submission file {output_filename}\")\n",
    "    final_submission_pd.to_csv(output_filename, index=False)\n",
    "    logger.info(f\"Kaggle submission file created successfully: {output_filename}\")\n",
    "    del submission_df, pred_series, submission_sorted, final_submission_gpu, final_submission_pd\n",
    "\n",
    "create_submission_file(df_test_ids, kaggle_predictions_np, \"gpu_submission.csv\")\n",
    "logger.info(\"--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
